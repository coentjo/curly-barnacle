{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QAhBW4_JFsv"
      },
      "source": [
        "# Chat with Self Help Concepts with RAG\n",
        "\n",
        "This notebook will guide you through the process of setting up the environment, importing documents, and interacting with LangChain for document-based Q&A. We'll cover topics such as document preprocessing, question formulation, and analyzing the model's responses.\n",
        "\n",
        "Whether you're a researcher, student, or professional, this demo notebook will showcase how LangChain can revolutionize your document exploration and information retrieval workflows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZ3ah-Fi99HT"
      },
      "source": [
        "# Prerequisites\n",
        "\n",
        "This code installs several Python packages that are required for the project. Explanations happily generated for you by [Chepetto](https://openai.com/blog/chatgpt).\n",
        "\n",
        "- [`langchain`](<https://python.langchain.com/>) is a package for language modeling and language generation tasks.\n",
        "- [`openai`](<https://openai.com/>) is a package for accessing the OpenAI API, which provides access to various language models and AI tools.\n",
        "- [`pypdf`](<https://pypi.org/project/PyPDF2/>) is a package for working with PDF files in Python.\n",
        "- [`tiktoken`](<https://github.com/openai/tiktoken>) is a package for accessing the TikTok API.\n",
        "- [`faiss-cpu`](<https://github.com/facebookresearch/faiss>) is a package for performing efficient similarity searches on large datasets using the FAISS library.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "dgzZqejc_tHY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -qU langchain openai pypdf tiktoken faiss-gpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_g3qnWtL9uoZ"
      },
      "source": [
        "# OpenAI API Key\n",
        "\n",
        "To use the OpenAI API, you need to obtain an API key from the [OpenAI website](https://platform.openai.com/account/api-keys). The API key is a unique identifier that allows you to access the OpenAI API and make requests to it. By setting the 'OPENAI_API_KEY' environment variable, you can securely provide your API key to the code without hardcoding it into the script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DAlyjKew6n0N"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "os.environ['OPENAI_API_KEY'] = getpass.getpass(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyqRRBW89pef"
      },
      "source": [
        "# Embeddings setup\n",
        "\n",
        "This code initializes an instance of the [OpenAIEmbeddings](https://python.langchain.com/en/latest/reference/modules/embeddings.html?highlight=embeddings#langchain.embeddings.OpenAIEmbeddings) class and assigns it to the variable embeddings. An [embedding](https://platform.openai.com/docs/guides/embeddings) is a way to represent words or phrases as numeric vectors, which can be used as input to machine learning models.  The `OpenAIEmbeddings` class provides access to pre-trained word embeddings from OpenAI, which were trained on a large corpus of text data using advanced deep learning techniques.\n",
        "\n",
        "Once you have initialized an instance of the `OpenAIEmbeddings` class, you can use it to obtain the embedding vector for any given chunk of text. This can be useful for a variety of [natural language processing](https://en.wikipedia.org/wiki/Natural_language_processing) (NLP) tasks, such as sentiment analysis, language translation, and text classification. In this notebook we use it to do [semantic search](https://en.wikipedia.org/wiki/Semantic_search) with a [vector database](https://www.youtube.com/watch?v=klTvEwg3oJ4&ab_channel=Fireship) in this case.\n",
        "\n",
        "## Model\n",
        "\n",
        "| Name | Tokenizer | Max input tokens | Output dimensions |\n",
        "| :--- | :--- | ---: | ---: |\n",
        "| text-embedding-ada-002 | cl100k_base | 8191 | 1536 |\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ywZY3ISvBtG0"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQndn4wG9gmV"
      },
      "source": [
        "# Splitter setup\n",
        "\n",
        "The [RecursiveCharacterTextSplitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter) is a text splitting tool that takes in a large text document as input and splits it into smaller chunks for downstream processing. Here's what each parameter in the splitter setup means:\n",
        "\n",
        "- `chunk_size`: This parameter specifies the size of each chunk of text that the splitter will output. In this case, the splitter is set up to output chunks of 500 characters each.\n",
        "\n",
        "- `chunk_overlap`: This parameter specifies the number of characters of overlap that each chunk will have with the next chunk. In this case, the splitter is set up to have an overlap of 20 characters between adjacent chunks.\n",
        "\n",
        "- `length_function`: This parameter specifies the function that the splitter will use to calculate the length of the input text. In this case, the `len` function is used, which returns the number of characters in the text.\n",
        "\n",
        "Together, these parameters determine how the input text will be split into smaller chunks. The splitter will output chunks of 500 characters each, with an overlap of 20 characters between adjacent chunks, until the entire input text has been processed. This setup is designed to balance the need for small enough chunks for efficient processing, with enough overlap between chunks to minimize the risk of losing contextual information at the boundaries between chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "oXNfZfbW-AwH"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 750,\n",
        "    chunk_overlap  = 50,\n",
        "    length_function = len,\n",
        "    is_separator_regex = False,\n",
        "    separators = ['.','!','?','\\n','\\n\\n'],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIrQCZBg9bjo"
      },
      "source": [
        "# Load (and split) documents\n",
        "\n",
        "This code snippet loads PDF files from a directory named \"pdf/\" using a [PyPDFDirectoryLoader](https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/pdf.html?highlight=PyPDFDirectoryLoader) class from the `langchain.document_loaders` module. The `loader` variable is an instance of `PyPDFDirectoryLoader`, which takes the directory path as an argument.\n",
        "\n",
        "After instantiating the loader, the code calls the `load_and_split` method to load the PDF files from the directory and split their text using the text splitter we created before.\n",
        "\n",
        "## Upload your PDFs\n",
        "Create a folder called 'pdf' and throw in any number of pdf's you'd like to chat with.\n",
        "\n",
        "> Note; The pdf's will be deleted once you close the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "PLbGxELv8X4C"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "788"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "\n",
        "loader = TextLoader(\"txt/The Power Of Habit - Charles Duhigg.txt\")\n",
        "docs = loader.load_and_split(text_splitter=text_splitter)\n",
        "len(docs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cleanup_chunk(chunk: str) -> str:\n",
        "    chunk.page_content = chunk.page_content.removeprefix(\". \")\n",
        "\n",
        "    if not chunk.page_content.endswith(\".\"):\n",
        "        chunk.page_content += \".\"\n",
        "\n",
        "    return chunk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "docs = [cleanup_chunk(chunk) for chunk in docs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "imTLGCkO14Qd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It tastes so good. After all, one dose of processed meat, salty fries, and sugary soda poses a relatively small health risk, right? It's not like you do it all the time. But habits emerge without our permission. Studies indicate that families usually don't intend to eat fast food on a regular basis. What happens is that a once-a-month pattern slowly becomes once a week, and then twice a week as the cues and rewards create a habit until the kids are consuming an unhealthy amount of hamburgers and fries.\n",
            "---\n",
            "When researchers at the University of North Texas and Yale tried to understand why families gradually increased their fast food consumption, they found a series of cues and rewards that most customers never knew were influencing their behaviors. They discovered the habit loop. Every McDonald's, for instance, looks the same the company deliberately tries to standardize stores architecture and what employees say to customers, so everything is a consistent cue to trigger eating routines.\n",
            "---\n",
            "The foods at some chains are specifically engineered to deliver immediate rewards the fries, for instance, are designed to begin disintegrating the moment they hit your tongue, in order to deliver a hit of salt and grease as fast as possible, causing your pleasure centers to light up and your brain to lock in the pattern. All the better for tightening the habit loop. However, even these habits are delicate. When a fast food restaurant closes down, the families that previously ate there will often start having dinner at home, rather than seek out an alternative location. Even small shifts can end the pattern. But since we often don't recognize these habit loops as they grow, we are blind to our ability to control them.\n",
            "---\n",
            "By learning to observe with cues and rewards, though, we can change the routines. By 2000, seven years after Eugene's illness, his life had achieved a kind of equilibrium. He went for a walk every morning. He ate what he wanted, sometimes five or six times a day. His wife knew that as long as the television was tuned to the History Channel, Eugene would settle into his plush chair and watch it regardless of whether it was airing reruns or new programs. He couldn't tell the difference. As he got older, however, Eugene's habits started impacting his life in negative ways. He was sedentary, sometimes watching television for hours at a time because he never grew bored with the shows. His physicians became worried about his heart.\n",
            "---\n",
            "His physicians became worried about his heart. The doctors told Beverly to keep him on a strict diet of healthy foods. She tried, but it was difficult to influence how frequently he ate or what he consumed. He never recalled her admonitions. Even if the refrigerator was stocked with fruits and vegetables, Eugene would root around until he found the bacon and eggs. That was his routine. And as Eugene aged and his bones became more brittle, the doctors said he needed to be more careful walking around. In his mind, however, Eugene was twenty years younger. He never remembered to step carefully. \"All my life I was fascinated by memory,\" Squire told me. \"Then I met E.P., and saw how rich life can be even if you can't remember it.\n",
            "---\n"
          ]
        }
      ],
      "source": [
        "chunk_testset = docs[88:93]\n",
        "\n",
        "for chunk in chunk_testset:\n",
        "    print(chunk.page_content)\n",
        "    print('---')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<Encoding 'cl100k_base'>\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "\n",
        "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "print(encoding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2181, 36263, 779, 1695, 13, 4740, 682, 11, 832, 19660, 315, 15590, 13339, 11, 74975, 53031, 11, 323, 31705, 661, 39962, 34103, 264, 12309, 2678, 2890, 5326, 11, 1314, 30, 1102, 596, 539, 1093, 499, 656, 433, 682, 279, 892, 13, 2030, 26870, 34044, 2085, 1057, 8041, 13, 19241, 13519, 430, 8689, 6118, 1541, 956, 30730, 311, 8343, 5043, 3691, 389, 264, 5912, 8197, 13, 3639, 8741, 374, 430, 264, 3131, 7561, 23086, 5497, 14297, 9221, 3131, 264, 2046, 11, 323, 1243, 11157, 264, 2046, 439, 279, 57016, 323, 21845, 1893, 264, 14464, 3156, 279, 6980, 527, 35208, 459, 53808, 3392, 315, 57947, 388, 323, 53031, 13]\n"
          ]
        }
      ],
      "source": [
        "chunk = chunk_testset[0].page_content\n",
        "num_tokens = encoding.encode(chunk)\n",
        "print(num_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "107 tokens, 507 characters.\n"
          ]
        }
      ],
      "source": [
        "print(f\"{len(num_tokens)} tokens, {len(chunk)} characters.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "107 tokens, 507 characters.\n",
            "83 tokens, 489 characters.\n",
            "142 tokens, 727 characters.\n",
            "154 tokens, 735 characters.\n",
            "155 tokens, 734 characters.\n",
            "---\n",
            "Total 641 tokens, 3192 characters.\n"
          ]
        }
      ],
      "source": [
        "total_chars = 0\n",
        "total_tokens = 0\n",
        "\n",
        "for chunk in chunk_testset:\n",
        "    num_chars = len(chunk.page_content)\n",
        "    total_chars += num_chars\n",
        "    num_tokens = len(encoding.encode(chunk.page_content))\n",
        "    total_tokens += num_tokens\n",
        "    print(f\"{num_tokens} tokens, {num_chars} characters.\") \n",
        "\n",
        "print('---')\n",
        "print(f\"Total {total_tokens} tokens, {total_chars} characters.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hq0xnWT79I0B"
      },
      "source": [
        "# Vector store setup\n",
        "\n",
        "ChromaDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "umIO18tb8tpz"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "db = FAISS.from_documents(docs, embeddings)\n",
        "\n",
        "query = \"What is procrastination?\"\n",
        "docs = db.similarity_search(query)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "When people marshal their will power to quit procrastinating, they often succeed, at first. Over time, though, their will power muscle starts to fade. The book they're supposed to be studying or the memo they're supposed to be writing gets boring, and the lure of Facebook grows stronger. Eventually, they give in. So most procrastination solutions ask people to pay close attention to how their resolve fades, when their willpower fails, and to accommodate that impulse, rather than ignore it. If you tend to give in to Facebook every 45 minutes or so, then go ahead and let yourself indulge the craving. 10 minutes. Set your watch and time yourself.\n",
            "{'source': 'txt/The Power Of Habit - Charles Duhigg.txt'}\n",
            "---\n",
            "Researchers have built on this finding to explain all sorts of phenomena. Some have suggested it helps clarify why otherwise successful people succumb to extramarital affairs, are most likely to start late at night after a long day of using willpower at work, or why good physicians make dumb mistakes, which most often occur after a doctor has finished a long, complicated task that requires intense focus. If you want to do something that requires willpower, like going for a run after work, you have to conserve your willpower muscle during the day, Moore event told me. If you use it up too early on tedious tasks like writing emails or filling out complicated and boring expense forms, all the strength will be gone by the time you get home.\n",
            "{'source': 'txt/The Power Of Habit - Charles Duhigg.txt'}\n",
            "---\n",
            "Research by James Prochaska at the University of Rhode Island and others shows that as smokers quit and then relapse, they begin to achieve a self-awareness about the cues and rewards that drive their smoking patterns. The first few times we fail to change, we're probably not aware why. However, as a pattern emerges, I'm usually good at resisting in the afternoon, but it's the mornings when I really struggle we start to understand, and analyze, what's really going on. We learn about ourselves sometimes without knowing we're learning, Prochaska told me. That's why failure is so valuable. It forces us to learn, even if we don't want to. Studies of procrastination suggest a similar dynamic.\n",
            "{'source': 'txt/The Power Of Habit - Charles Duhigg.txt'}\n",
            "---\n",
            "Some are simple, you automatically put toothpaste on your toothbrush before sticking it in your mouth. Some, such as getting dressed or making the kids lunch, are a little more complex. Others are so complicated that it's remarkable a small bit of tissue that evolved millions of years ago can turn them into habits at all. the act of backing your car out of the driveway.\n",
            "{'source': 'txt/The Power Of Habit - Charles Duhigg.txt'}\n",
            "---\n"
          ]
        }
      ],
      "source": [
        "query = \"What is procrastination?\"\n",
        "answer = db.similarity_search(query)\n",
        "for a in answer:\n",
        "    print(a.page_content)\n",
        "    print(a.metadata)\n",
        "    print('---')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osmwb3QL9QiX"
      },
      "source": [
        "# Save the db\n",
        "\n",
        "This code saves the FAISS index created in the previous code cell to disk with the name `faiss_index`. The `save_local()` method is called on the `faiss_index` object, which is the FAISS index created earlier. The `save_local()` method is a utility method provided by the FAISS class to save the index to the local file system.\n",
        "\n",
        "After executing this code, a file named \"faiss_index\" should be created in the current working directory. This file contains the serialized version of the FAISS index, which can be loaded back into memory later using the `FAISS.load_local()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "M3AVw_PWCEg-"
      },
      "outputs": [],
      "source": [
        "db.save_local(\"faiss_index_pdev_books\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5sRA2Fe9Tky"
      },
      "source": [
        "# Test the vector store\n",
        "\n",
        "This code performs a similarity search using the FAISS index created earlier and the query string *\"Is a prototyping more than enough for software?\"*.\n",
        "\n",
        "The `similarity_search()` method is called on the faiss_index object with two arguments: the query string and `k=5`, which specifies that the top 5 most similar documents should be returned. The result of the similarity search is stored in the `query_result` variable.\n",
        "\n",
        "The code then iterates over the chunks in the `query_result` list and prints the metadata and page content of each `chunk`. Specifically, it prints the page number and source of the document, along with its page content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "lV2Pmvrt_FTO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'source': 'txt/The Power Of Habit - Charles Duhigg.txt'}: When people marshal their will power to quit procrastinating, they often succeed, at first. Over time, though, their will power muscle starts to fade. The book they're supposed to be studying or the memo they're supposed to be writing gets boring, and the lure of Facebook grows stronger. Eventually, they give in. So most procrastination solutions ask people to pay close attention to how their resolve fades, when their willpower fails, and to accommodate that impulse, rather than ignore it. If you tend to give in to Facebook every 45 minutes or so, then go ahead and let yourself indulge the craving. 10 minutes. Set your watch and time yourself.\n",
            "{'source': 'txt/The Power Of Habit - Charles Duhigg.txt'}: Researchers have built on this finding to explain all sorts of phenomena. Some have suggested it helps clarify why otherwise successful people succumb to extramarital affairs, are most likely to start late at night after a long day of using willpower at work, or why good physicians make dumb mistakes, which most often occur after a doctor has finished a long, complicated task that requires intense focus. If you want to do something that requires willpower, like going for a run after work, you have to conserve your willpower muscle during the day, Moore event told me. If you use it up too early on tedious tasks like writing emails or filling out complicated and boring expense forms, all the strength will be gone by the time you get home.\n",
            "{'source': 'txt/The Power Of Habit - Charles Duhigg.txt'}: Research by James Prochaska at the University of Rhode Island and others shows that as smokers quit and then relapse, they begin to achieve a self-awareness about the cues and rewards that drive their smoking patterns. The first few times we fail to change, we're probably not aware why. However, as a pattern emerges, I'm usually good at resisting in the afternoon, but it's the mornings when I really struggle we start to understand, and analyze, what's really going on. We learn about ourselves sometimes without knowing we're learning, Prochaska told me. That's why failure is so valuable. It forces us to learn, even if we don't want to. Studies of procrastination suggest a similar dynamic.\n",
            "{'source': 'txt/The Power Of Habit - Charles Duhigg.txt'}: Some are simple, you automatically put toothpaste on your toothbrush before sticking it in your mouth. Some, such as getting dressed or making the kids lunch, are a little more complex. Others are so complicated that it's remarkable a small bit of tissue that evolved millions of years ago can turn them into habits at all. the act of backing your car out of the driveway.\n"
          ]
        }
      ],
      "source": [
        "query_result = db.similarity_search(\"What is procrastination?\", k=4)\n",
        "\n",
        "for chunk in query_result:\n",
        "    print(f\"{chunk.metadata}: {chunk.page_content[:]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncBRUXkX8xrH"
      },
      "source": [
        "# Chat memory\n",
        "\n",
        "This code imports the [ConversationBufferWindowMemory](https://python.langchain.com/en/latest/modules/memory/types/buffer_window.html) class from the `langchain.memory` module and creates an instance of it called `memory`. This class represents a memory buffer that stores conversations in a windowed fashion, meaning that the buffer only retains a certain number of recent conversations.\n",
        "\n",
        "The constructor of the `ConversationBufferWindowMemory` class takes two arguments: `memory_key` and `return_messages`. The `memory_key` parameter specifies a unique identifier for the memory buffer, and the `return_messages` parameter indicates whether or not to return the stored messages along with their metadata when accessing the memory buffer.\n",
        "\n",
        "In this code, the `memory_key` is set to \"chat_history\", which is being used to store the chat conversations. The return_messages parameter is set to `True`, which indicates that the stored messages will be returned along with their metadata when accessing the memory buffer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "hWbEn5NlBWK7"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "memory = ConversationBufferWindowMemory(memory_key=\"chat_history\", return_messages=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrkpI1EY8t02"
      },
      "source": [
        "# Chain setup\n",
        "\n",
        "This code imports several classes and functions from various modules in the langchain package and creates an instance of the [ConversationalRetrievalChain](https://python.langchain.com/en/latest/modules/chains/index_examples/chat_vector_db.html?highlight=ConversationalRetrievalChain) class called `qa`.\n",
        "\n",
        "The `ConversationalRetrievalChain` class is a high-level class that provides an interface for building a conversational agent that can perform retrieval-based question answering. In this code, the `qa` instance is initialized using the `from_llm()` method, which initializes the agent using an LLM model, a retriever and the memory buffer.\n",
        "\n",
        "### LLM\n",
        "The `OpenAI` class from the `langchain.llms` module represents an instance of the OpenAI language model. In this code, an instance of the OpenAI class is created of the model \"[gpt-3.5-turbo](https://platform.openai.com/docs/models)\".\n",
        "\n",
        "### Vector Store\n",
        "The `faiss_index.as_retriever()` method returns a retriever instance that wraps the FAISS index created earlier. This retriever is used to retrieve candidate answers to questions asked of the conversational agent.\n",
        "\n",
        "### Chat History Memory\n",
        "The `memory` variable is a memory buffer that was created earlier using the `ConversationBufferWindowMemory` class. This memory buffer is used to store and retrieve past conversations for use in future interactions.\n",
        "\n",
        "The `verbose=True` parameter indicates that verbose output should be produced when running the conversational agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "--P_eVczGm1X"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import ConversationalRetrievalChain, LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.chains import RetrievalQA, RetrievalQAWithSourcesChain\n",
        "\n",
        "qa = ConversationalRetrievalChain.from_llm(\n",
        "    OpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7, max_tokens=1000),\n",
        "    db.as_retriever(k=4),\n",
        "     verbose=True) #memory=memory\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZykRyF1TWqsc"
      },
      "source": [
        "# Ask away\n",
        "\n",
        "This code snippet involves a conversational agent that performs question-answering tasks. The user inputs a question, which is passed to the agent as a dictionary with a \"question\" key.\n",
        "\n",
        "The agent then creates an embedding of the question to query the FAISS index and retrieve relevant text chunks based on an internal ranking criterion.\n",
        "\n",
        "Next, the agent makes two calls to the LLM model (\"gpt-3.5-turbo\").\n",
        "\n",
        "- The first call uses the retrieved text chunks, chat history, and the current user question to prompt the LLM to come up with a 'better' question for the entire context.\n",
        "- The second call uses the enhanced question to retrieve the actual answer to the original user question.\n",
        "\n",
        "The resulting answer is stored in the 'chat_result variable', which contains metadata and content related to the answer. The actual answer can be accessed using the \"answer\" key of the 'chat_result' dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "Qb5zcHgA6qDY"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Missing some input keys: {'chat_history'}",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[79], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m query \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mwhat are possible ways to counter procrastination?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[39m#query = \"Wie is Marcus Quintillianus?\"\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m chat_result \u001b[39m=\u001b[39m qa({\u001b[39m\"\u001b[39;49m\u001b[39mquestion\u001b[39;49m\u001b[39m\"\u001b[39;49m: query})\n\u001b[1;32m      5\u001b[0m chat_result\n",
            "File \u001b[0;32m/workspaces/curly-barnacle/.venv/lib/python3.10/site-packages/langchain/chains/base.py:259\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\n\u001b[1;32m    225\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    226\u001b[0m     inputs: Union[Dict[\u001b[39mstr\u001b[39m, Any], Any],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    232\u001b[0m     include_run_info: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    233\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Any]:\n\u001b[1;32m    234\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \n\u001b[1;32m    236\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[39m            `Chain.output_keys`.\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 259\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprep_inputs(inputs)\n\u001b[1;32m    260\u001b[0m     callback_manager \u001b[39m=\u001b[39m CallbackManager\u001b[39m.\u001b[39mconfigure(\n\u001b[1;32m    261\u001b[0m         callbacks,\n\u001b[1;32m    262\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata,\n\u001b[1;32m    268\u001b[0m     )\n\u001b[1;32m    269\u001b[0m     new_arg_supported \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39msignature(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[0;32m/workspaces/curly-barnacle/.venv/lib/python3.10/site-packages/langchain/chains/base.py:413\u001b[0m, in \u001b[0;36mChain.prep_inputs\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    411\u001b[0m     external_context \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory\u001b[39m.\u001b[39mload_memory_variables(inputs)\n\u001b[1;32m    412\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mexternal_context)\n\u001b[0;32m--> 413\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_inputs(inputs)\n\u001b[1;32m    414\u001b[0m \u001b[39mreturn\u001b[39;00m inputs\n",
            "File \u001b[0;32m/workspaces/curly-barnacle/.venv/lib/python3.10/site-packages/langchain/chains/base.py:171\u001b[0m, in \u001b[0;36mChain._validate_inputs\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    169\u001b[0m missing_keys \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_keys)\u001b[39m.\u001b[39mdifference(inputs)\n\u001b[1;32m    170\u001b[0m \u001b[39mif\u001b[39;00m missing_keys:\n\u001b[0;32m--> 171\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing some input keys: \u001b[39m\u001b[39m{\u001b[39;00mmissing_keys\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
            "\u001b[0;31mValueError\u001b[0m: Missing some input keys: {'chat_history'}"
          ]
        }
      ],
      "source": [
        "query = \"what are possible ways to counter procrastination?\"\n",
        "#query = \"Wie is Marcus Quintillianus?\"\n",
        "\n",
        "chat_result = qa({\"question\": query})\n",
        "chat_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
