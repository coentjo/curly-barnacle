{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QAhBW4_JFsv"
      },
      "source": [
        "# Chat with Self Help Concepts with RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZ3ah-Fi99HT"
      },
      "source": [
        "## Prerequisites\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dgzZqejc_tHY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: langchain in /home/vscode/.local/lib/python3.10/site-packages (0.0.271)\n",
            "Requirement already satisfied: tiktoken in /home/vscode/.local/lib/python3.10/site-packages (0.4.0)\n",
            "Requirement already satisfied: unstructured in /home/vscode/.local/lib/python3.10/site-packages (0.10.5)\n",
            "Requirement already satisfied: openai in /home/vscode/.local/lib/python3.10/site-packages (0.27.9)\n",
            "Requirement already satisfied: tqdm in /home/vscode/.local/lib/python3.10/site-packages (4.66.1)\n",
            "Requirement already satisfied: chromadb in /home/vscode/.local/lib/python3.10/site-packages (0.4.6)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /home/vscode/.local/lib/python3.10/site-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/vscode/.local/lib/python3.10/site-packages (from langchain) (2.0.20)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/vscode/.local/lib/python3.10/site-packages (from langchain) (3.8.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/vscode/.local/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /home/vscode/.local/lib/python3.10/site-packages (from langchain) (0.5.14)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.21 in /home/vscode/.local/lib/python3.10/site-packages (from langchain) (0.0.26)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /home/vscode/.local/lib/python3.10/site-packages (from langchain) (2.8.5)\n",
            "Requirement already satisfied: numpy<2,>=1 in /home/vscode/.local/lib/python3.10/site-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /home/vscode/.local/lib/python3.10/site-packages (from langchain) (1.10.12)\n",
            "Requirement already satisfied: requests<3,>=2 in /home/vscode/.local/lib/python3.10/site-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/vscode/.local/lib/python3.10/site-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /home/vscode/.local/lib/python3.10/site-packages (from tiktoken) (2023.8.8)\n",
            "Requirement already satisfied: chardet in /home/vscode/.local/lib/python3.10/site-packages (from unstructured) (5.2.0)\n",
            "Requirement already satisfied: filetype in /home/vscode/.local/lib/python3.10/site-packages (from unstructured) (1.2.0)\n",
            "Requirement already satisfied: python-magic in /home/vscode/.local/lib/python3.10/site-packages (from unstructured) (0.4.27)\n",
            "Requirement already satisfied: lxml in /home/vscode/.local/lib/python3.10/site-packages (from unstructured) (4.9.3)\n",
            "Requirement already satisfied: nltk in /home/vscode/.local/lib/python3.10/site-packages (from unstructured) (3.8.1)\n",
            "Requirement already satisfied: tabulate in /home/vscode/.local/lib/python3.10/site-packages (from unstructured) (0.9.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /home/vscode/.local/lib/python3.10/site-packages (from unstructured) (4.12.2)\n",
            "Requirement already satisfied: emoji in /home/vscode/.local/lib/python3.10/site-packages (from unstructured) (2.8.0)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.2 in /home/vscode/.local/lib/python3.10/site-packages (from chromadb) (0.7.2)\n",
            "Requirement already satisfied: fastapi<0.100.0,>=0.95.2 in /home/vscode/.local/lib/python3.10/site-packages (from chromadb) (0.99.1)\n",
            "Requirement already satisfied: uvicorn[standard]>=0.18.3 in /home/vscode/.local/lib/python3.10/site-packages (from chromadb) (0.23.2)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /home/vscode/.local/lib/python3.10/site-packages (from chromadb) (3.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /home/vscode/.local/lib/python3.10/site-packages (from chromadb) (4.7.1)\n",
            "Requirement already satisfied: pulsar-client>=3.1.0 in /home/vscode/.local/lib/python3.10/site-packages (from chromadb) (3.2.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /home/vscode/.local/lib/python3.10/site-packages (from chromadb) (1.15.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /home/vscode/.local/lib/python3.10/site-packages (from chromadb) (0.13.3)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /home/vscode/.local/lib/python3.10/site-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /home/vscode/.local/lib/python3.10/site-packages (from chromadb) (7.4.0)\n",
            "Requirement already satisfied: importlib-resources in /home/vscode/.local/lib/python3.10/site-packages (from chromadb) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/vscode/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/vscode/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/vscode/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /home/vscode/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/vscode/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/vscode/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/vscode/.local/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/vscode/.local/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /home/vscode/.local/lib/python3.10/site-packages (from fastapi<0.100.0,>=0.95.2->chromadb) (0.27.0)\n",
            "Requirement already satisfied: coloredlogs in /home/vscode/.local/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /home/vscode/.local/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
            "Requirement already satisfied: packaging in /home/vscode/.local/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (23.1)\n",
            "Requirement already satisfied: protobuf in /home/vscode/.local/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (4.24.1)\n",
            "Requirement already satisfied: sympy in /home/vscode/.local/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
            "Requirement already satisfied: six>=1.5 in /home/vscode/.local/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: monotonic>=1.5 in /home/vscode/.local/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /home/vscode/.local/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: python-dateutil>2.1 in /home/vscode/.local/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: certifi in /home/vscode/.local/lib/python3.10/site-packages (from pulsar-client>=3.1.0->chromadb) (2023.7.22)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/vscode/.local/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/vscode/.local/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /home/vscode/.local/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: click>=7.0 in /home/vscode/.local/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /home/vscode/.local/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /home/vscode/.local/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.0)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /home/vscode/.local/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /home/vscode/.local/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.17.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /home/vscode/.local/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /home/vscode/.local/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (11.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /home/vscode/.local/lib/python3.10/site-packages (from beautifulsoup4->unstructured) (2.4.1)\n",
            "Requirement already satisfied: joblib in /home/vscode/.local/lib/python3.10/site-packages (from nltk->unstructured) (1.3.2)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /home/vscode/.local/lib/python3.10/site-packages (from starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (3.7.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/vscode/.local/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /home/vscode/.local/lib/python3.10/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /home/vscode/.local/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /home/vscode/.local/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /home/vscode/.local/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (1.1.3)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -qU langchain tiktoken unstructured openai tqdm chromadb  # NOTE: Chroma needs Python 3.10.x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_g3qnWtL9uoZ"
      },
      "source": [
        "# OpenAI API Key\n",
        "\n",
        "To use the OpenAI API, you need to obtain an API key from the [OpenAI website](https://platform.openai.com/account/api-keys). The API key is a unique identifier that allows you to access the OpenAI API and make requests to it. By setting the 'OPENAI_API_KEY' environment variable, you can securely provide your API key to the code without hardcoding it into the script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DAlyjKew6n0N"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "os.environ['OPENAI_API_KEY'] = getpass.getpass(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyqRRBW89pef"
      },
      "source": [
        "# Embeddings setup\n",
        "\n",
        "This code initializes an instance of the [OpenAIEmbeddings](https://python.langchain.com/en/latest/reference/modules/embeddings.html?highlight=embeddings#langchain.embeddings.OpenAIEmbeddings) class and assigns it to the variable embeddings. An [embedding](https://platform.openai.com/docs/guides/embeddings) is a way to represent words or phrases as numeric vectors, which can be used as input to machine learning models.  The `OpenAIEmbeddings` class provides access to pre-trained word embeddings from OpenAI, which were trained on a large corpus of text data using advanced deep learning techniques.\n",
        "\n",
        "Once you have initialized an instance of the `OpenAIEmbeddings` class, you can use it to obtain the embedding vector for any given chunk of text. This can be useful for a variety of [natural language processing](https://en.wikipedia.org/wiki/Natural_language_processing) (NLP) tasks, such as sentiment analysis, language translation, and text classification. In this notebook we use it to do [semantic search](https://en.wikipedia.org/wiki/Semantic_search) with a [vector database](https://www.youtube.com/watch?v=klTvEwg3oJ4&ab_channel=Fireship) in this case.\n",
        "\n",
        "## Model\n",
        "\n",
        "| Name | Tokenizer | Max input tokens | Output dimensions |\n",
        "| :--- | :--- | ---: | ---: |\n",
        "| text-embedding-ada-002 | cl100k_base | 8191 | 1536 |\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ywZY3ISvBtG0"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQndn4wG9gmV"
      },
      "source": [
        "# Splitter setup\n",
        "\n",
        "The [RecursiveCharacterTextSplitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter) is a text splitting tool that takes in a large text document as input and splits it into smaller chunks for downstream processing. Here's what each parameter in the splitter setup means:\n",
        "\n",
        "- `chunk_size`: This parameter specifies the size of each chunk of text that the splitter will output. In this case, the splitter is set up to output chunks of 500 characters each.\n",
        "\n",
        "- `chunk_overlap`: This parameter specifies the number of characters of overlap that each chunk will have with the next chunk. In this case, the splitter is set up to have an overlap of 20 characters between adjacent chunks.\n",
        "\n",
        "- `length_function`: This parameter specifies the function that the splitter will use to calculate the length of the input text. In this case, the `len` function is used, which returns the number of characters in the text.\n",
        "\n",
        "Together, these parameters determine how the input text will be split into smaller chunks. The splitter will output chunks of 500 characters each, with an overlap of 20 characters between adjacent chunks, until the entire input text has been processed. This setup is designed to balance the need for small enough chunks for efficient processing, with enough overlap between chunks to minimize the risk of losing contextual information at the boundaries between chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "oXNfZfbW-AwH"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 750,\n",
        "    chunk_overlap  = 50,\n",
        "    length_function = len,\n",
        "    is_separator_regex = False,\n",
        "    separators = ['.','!','?','\\n','\\n\\n'],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIrQCZBg9bjo"
      },
      "source": [
        "# Load (and split) documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "PLbGxELv8X4C"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 6/6 [00:00<00:00, 661.39it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "3914"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.document_loaders import TextLoader, DirectoryLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "text_loader_kwargs={'autodetect_encoding': True}\n",
        "loader = DirectoryLoader('txt/', loader_cls=TextLoader, glob='**/*.txt', show_progress=True, loader_kwargs=text_loader_kwargs)\n",
        "\n",
        "docs = loader.load_and_split(text_splitter=text_splitter)\n",
        "len(docs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(page_content='. It\\'s so fulfilling now when a new artist plays me their single and says, \"This is going to be the next HAYA.\" After Andrew Poll built his pregnancy prediction machine, after he identified hundreds of thousands of female shoppers who were probably pregnant, after someone pointed out that some, in fact, most, of those women might be a little upset if they received an advertisement making it obvious Target knew their reproductive status. Everyone decided to take a step back and consider their options. The marketing department thought it might be wise to conduct a few small experiments before rolling out a national campaign', metadata={'source': 'txt/The Power Of Habit - Charles Duhigg.txt'})"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs[555]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cleanup_chunk(chunk: str) -> str:\n",
        "    chunk.page_content = chunk.page_content.removeprefix(\". \")\n",
        "\n",
        "    # push start to end dots\n",
        "    if not chunk.page_content.endswith(\".\"):\n",
        "        chunk.page_content += \".\"\n",
        "        \n",
        "    # damn newline bastards\n",
        "    chunk.page_content = chunk.page_content.replace('\\n', ' ')\n",
        "    \n",
        "    # remove the emptiness from it all\n",
        "    while chunk.page_content.find('  ') != -1:\n",
        "        chunk.page_content = chunk.page_content.replace('  ', ' ')\n",
        "\n",
        "    return chunk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3914/3914 [00:00<00:00, 220423.83it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Document(page_content='It\\'s so fulfilling now when a new artist plays me their single and says, \"This is going to be the next HAYA.\" After Andrew Poll built his pregnancy prediction machine, after he identified hundreds of thousands of female shoppers who were probably pregnant, after someone pointed out that some, in fact, most, of those women might be a little upset if they received an advertisement making it obvious Target knew their reproductive status. Everyone decided to take a step back and consider their options. The marketing department thought it might be wise to conduct a few small experiments before rolling out a national campaign.', metadata={'source': 'txt/The Power Of Habit - Charles Duhigg.txt'})"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs = [cleanup_chunk(chunk) for chunk in tqdm(docs)]\n",
        "docs[555]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "imTLGCkO14Qd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It tastes so good. After all, one dose of processed meat, salty fries, and sugary soda poses a relatively small health risk, right? It's not like you do it all the time. But habits emerge without our permission. Studies indicate that families usually don't intend to eat fast food on a regular basis. What happens is that a once-a-month pattern slowly becomes once a week, and then twice a week as the cues and rewards create a habit until the kids are consuming an unhealthy amount of hamburgers and fries.\n",
            "---\n",
            "When researchers at the University of North Texas and Yale tried to understand why families gradually increased their fast food consumption, they found a series of cues and rewards that most customers never knew were influencing their behaviors. They discovered the habit loop. Every McDonald's, for instance, looks the same the company deliberately tries to standardize stores architecture and what employees say to customers, so everything is a consistent cue to trigger eating routines.\n",
            "---\n",
            "The foods at some chains are specifically engineered to deliver immediate rewards the fries, for instance, are designed to begin disintegrating the moment they hit your tongue, in order to deliver a hit of salt and grease as fast as possible, causing your pleasure centers to light up and your brain to lock in the pattern. All the better for tightening the habit loop. However, even these habits are delicate. When a fast food restaurant closes down, the families that previously ate there will often start having dinner at home, rather than seek out an alternative location. Even small shifts can end the pattern. But since we often don't recognize these habit loops as they grow, we are blind to our ability to control them.\n",
            "---\n",
            "By learning to observe with cues and rewards, though, we can change the routines. By 2000, seven years after Eugene's illness, his life had achieved a kind of equilibrium. He went for a walk every morning. He ate what he wanted, sometimes five or six times a day. His wife knew that as long as the television was tuned to the History Channel, Eugene would settle into his plush chair and watch it regardless of whether it was airing reruns or new programs. He couldn't tell the difference. As he got older, however, Eugene's habits started impacting his life in negative ways. He was sedentary, sometimes watching television for hours at a time because he never grew bored with the shows. His physicians became worried about his heart.\n",
            "---\n",
            "His physicians became worried about his heart. The doctors told Beverly to keep him on a strict diet of healthy foods. She tried, but it was difficult to influence how frequently he ate or what he consumed. He never recalled her admonitions. Even if the refrigerator was stocked with fruits and vegetables, Eugene would root around until he found the bacon and eggs. That was his routine. And as Eugene aged and his bones became more brittle, the doctors said he needed to be more careful walking around. In his mind, however, Eugene was twenty years younger. He never remembered to step carefully. \"All my life I was fascinated by memory,\" Squire told me. \"Then I met E.P., and saw how rich life can be even if you can't remember it.\n",
            "---\n"
          ]
        }
      ],
      "source": [
        "chunk_testset = docs[88:93]\n",
        "\n",
        "for chunk in chunk_testset:\n",
        "    print(chunk.page_content)\n",
        "    print('---')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<Encoding 'cl100k_base'>\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "\n",
        "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "print(encoding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2181, 36263, 779, 1695, 13, 4740, 682, 11, 832, 19660, 315, 15590, 13339, 11, 74975, 53031, 11, 323, 31705, 661, 39962, 34103, 264, 12309, 2678, 2890, 5326, 11, 1314, 30, 1102, 596, 539, 1093, 499, 656, 433, 682, 279, 892, 13, 2030, 26870, 34044, 2085, 1057, 8041, 13, 19241, 13519, 430, 8689, 6118, 1541, 956, 30730, 311, 8343, 5043, 3691, 389, 264, 5912, 8197, 13, 3639, 8741, 374, 430, 264, 3131, 7561, 23086, 5497, 14297, 9221, 3131, 264, 2046, 11, 323, 1243, 11157, 264, 2046, 439, 279, 57016, 323, 21845, 1893, 264, 14464, 3156, 279, 6980, 527, 35208, 459, 53808, 3392, 315, 57947, 388, 323, 53031, 13]\n"
          ]
        }
      ],
      "source": [
        "chunk = chunk_testset[0].page_content\n",
        "num_tokens = encoding.encode(chunk)\n",
        "print(num_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "107 tokens, 507 characters.\n"
          ]
        }
      ],
      "source": [
        "print(f\"{len(num_tokens)} tokens, {len(chunk)} characters.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "107 tokens, 507 characters.\n",
            "83 tokens, 489 characters.\n",
            "142 tokens, 727 characters.\n",
            "154 tokens, 735 characters.\n",
            "155 tokens, 734 characters.\n",
            "---\n",
            "Total 641 tokens, 3192 characters.\n"
          ]
        }
      ],
      "source": [
        "total_chars = 0\n",
        "total_tokens = 0\n",
        "\n",
        "for chunk in chunk_testset:\n",
        "    num_chars = len(chunk.page_content)\n",
        "    total_chars += num_chars\n",
        "    num_tokens = len(encoding.encode(chunk.page_content))\n",
        "    total_tokens += num_tokens\n",
        "    print(f\"{num_tokens} tokens, {num_chars} characters.\") \n",
        "\n",
        "print('---')\n",
        "print(f\"Total {total_tokens} tokens, {total_chars} characters.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hq0xnWT79I0B"
      },
      "source": [
        "# Vector store setup\n",
        "\n",
        "ChromaDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "umIO18tb8tpz"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'Chroma' object has no attribute 'similarity_earch'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[24], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m db \u001b[39m=\u001b[39m Chroma\u001b[39m.\u001b[39mfrom_documents(docs, embeddings, persist_directory\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./chroma_db\u001b[39m\u001b[39m\"\u001b[39m, collection_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpdev-books\u001b[39m\u001b[39m'\u001b[39m, ids\u001b[39m=\u001b[39mids)\n\u001b[1;32m      9\u001b[0m query \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mWhat is procrastination?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 10\u001b[0m docs \u001b[39m=\u001b[39m db\u001b[39m.\u001b[39;49msimilarity_earch(query)\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Chroma' object has no attribute 'similarity_earch'"
          ]
        }
      ],
      "source": [
        "import uuid\n",
        "from langchain.vectorstores import Chroma \n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "ids = [str(uuid.uuid4()) for i in range(1, len(docs) + 1)] # random UUIDs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Load from disk\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#db = Chroma.from_documents(docs, embeddings, persist_directory=\"./chroma_db\", collection_name='pdev-books', ids=ids)\n",
        "\n",
        "query = \"What is procrastination?\"\n",
        "docs = db.similarity_search(query)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "page_content=\"In this book, however, I'm interested in his commitment to the following skill, which almost certainly played a key role in his accomplishments. Deep Work. activities performed in a state of distraction-free concentration that push your cognitive capabilities to their limit. These efforts create new value, improve your skill, and are hard to replicate. Deep work is necessary to wring every last drop of value out of your current intellectual capacity. We now know from decades of research, in both psychology and neuroscience, that the state of mental strain that accompanies deep work is also necessary to improve your abilities.\" metadata={'source': 'txt/Deep Work - Cal Newport.txt'}\n",
            "---\n",
            "page_content=\"The Deep Work Hypothesis. The ability to perform deep work is becoming increasingly rare at exactly the same time it is becoming increasingly valuable in our economy. As a consequence, the few who cultivate this skill and then make it the core of their working life will thrive. This book has two goals pursued in two parts. The first, tackled in Part 1, is to convince you that the deep work hypothesis is true. The second, tackled in Part 2, is to teach you how to take advantage of this reality by training your brain and transforming your work habits to place deep work at the core of your professional life. Before diving into these details, however, I'll take a moment to explain how I became such a devotee of depth.\" metadata={'source': 'txt/Deep Work - Cal Newport.txt'}\n",
            "---\n",
            "page_content='The sections ahead provide this closer look, and by doing so will help this connection between deep work and economic success shift for you from unexpected to unimpeachable. Deep work helps you quickly learn hard things. Let your mind become a lens, thanks to the converging rays of attention. Let your soul be all intent on whatever it is that is established in your mind as a dominant, wholly absorbing idea. This advice comes from Antonin D\\'Almei-Sertéange, a Dominican friar and professor of moral philosophy, who during the early part of the 20th century penned a slim but influential volume titled \"The Intellectual Life.' metadata={'source': 'txt/Deep Work - Cal Newport.txt'}\n",
            "---\n",
            "page_content=\"Assuming the trends outlined here continue, depth will become increasingly rare and therefore increasingly valuable. Having just established that there's nothing fundamentally flawed about deep work and nothing fundamentally necessary about the distracting behaviors that displace it, you can therefore continue with confidence with the ultimate goal of this book - to systematically develop your personal ability to go deep, and by doing so reap great rewards. Chapter 3. Deep Work is Meaningful. Rick Furrer is a blacksmith. He specializes in ancient and medieval metalworking practices, which he painstakingly recreates in his shop, Door County Forge Works.\" metadata={'source': 'txt/Deep Work - Cal Newport.txt'}\n",
            "---\n"
          ]
        }
      ],
      "source": [
        "query = \"What is deep work?\"\n",
        "answer = db.similarity_search(query)\n",
        "for a in answer:\n",
        "    print(a)\n",
        "    print('---')\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(Document(page_content=\"So I have invented another myth for myself, that I'm irresponsible. I'm actively irresponsible. I tell everyone I don't do anything. If anyone asks me to be on a committee for admissions, no, I tell them, I'm irresponsible. Feynman was adamant in avoiding administrative duties, because he knew they would only decrease his ability to do the one thing that mattered most in his professional life – to do real good physics work. Feynman, we can assume, was probably bad at responding to emails, and would likely switch universities if you would try to move him into an open office or demand that he tweet. about what matters provides clarity about what does not.\", metadata={'source': 'txt/Deep Work - Cal Newport.txt'}),\n",
              "  0.37731269001960754),\n",
              " (Document(page_content=\"People would have worked with me, coached me. Or, the system let me get away with more and more. I really liked it less and less. He got mad at the system. Hi there, John. This was your life. Ever think of taking responsibility? No, because in the fixed mindset, you don't take control of your abilities and your motivation. You look for your talent to carry you through, and when it doesn't, well then, what else could you have done? You are not a work in progress, you're a finished product. And finished products have to protect themselves, lament, and blame. Everything but take charge.\", metadata={'source': 'txt/Mindset - Carol Dweck.txt'}),\n",
              "  0.3785416781902313),\n",
              " (Document(page_content='You are less likely to procrastinate or give up because there is an immediate cost. If you don\\'t follow through, perhaps they\\'ll see you as untrustworthy or lazy. Suddenly, you are not only failing to uphold your promises to yourself, but also failing to uphold your promises to others. You can even automate this process. Thomas Frank, an entrepreneur in Boulder, Colorado, wakes up at 5.55 each morning. And if he doesn\\'t, he has a tweet automatically scheduled that says \"It\\'s 6/10 and I\\'m not up because I\\'m lazy.\" Reply to this for $5 via PayPal, limit 5, assuming my alarm didn\\'t malfunction. We are always trying to present our best selves to the world.', metadata={'source': 'txt/Atomic Habits - James Clear.txt'}),\n",
              "  0.37944188714027405),\n",
              " (Document(page_content=\"Returning to my own example, it's a similar commitment that enables me to succeed with fixed scheduling. I too am incredibly cautious about my use of the most dangerous word in one's productivity vocabulary. Yes. It takes a lot to convince me to agree to something that yields shallow work. If you ask for my involvement in university business that's not absolutely necessary, I might respond with a defense I learned from the department chair who hired me. Talk to me after tenure. Another tactic that works well for me is to be clear in my refusal, but ambiguous in my explanation for the refusal. The key is to avoid providing enough specificity about the excuse that the requester has the opportunity to diffuse it.\", metadata={'source': 'txt/Deep Work - Cal Newport.txt'}),\n",
              "  0.38006070256233215)]"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "db.similarity_search_with_score('Am I too lazy to take responsibility for a project?') # score is cosine distance - lower the better"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5sRA2Fe9Tky"
      },
      "source": [
        "# Test the vector store\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "lV2Pmvrt_FTO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'source': 'txt/Atomic Habits - James Clear.txt'}: Most of us are experts at avoiding criticism. It doesn't feel good to fail or to be judged publicly, so we tend to avoid situations where that might happen. And that's the biggest reason why you slip into motion rather than taking action. You want to delay failure. It's easy to be in motion and convince yourself that you're still making progress. You think, \"I've got conversations going with four potential clients right now. This is good. We're moving in the right direction.\" Or, \"I brainstormed some ideas for that book I want to write. This is coming together.\" Motion makes you feel like you're getting things done. But really, you're just preparing to get something done.\n",
            "{'source': 'txt/The Power Of Habit - Charles Duhigg.txt'}: When people marshal their will power to quit procrastinating, they often succeed, at first. Over time, though, their will power muscle starts to fade. The book they're supposed to be studying or the memo they're supposed to be writing gets boring, and the lure of Facebook grows stronger. Eventually, they give in. So most procrastination solutions ask people to pay close attention to how their resolve fades, when their willpower fails, and to accommodate that impulse, rather than ignore it. If you tend to give in to Facebook every 45 minutes or so, then go ahead and let yourself indulge the craving. 10 minutes. Set your watch and time yourself.\n",
            "{'source': 'txt/Atomic Habits - James Clear.txt'}: When friction is low, habits are easy. Increase the friction associated with bad behaviors. When friction is high, habits are difficult. Prime your environment to make future actions easier. Chapter 13 How to Stop Procrastinating by Using the Two-Minute Rule Twyla Tharp IS widely regarded as one of the greatest dancers and choreographers of the modern era. In 1992, she was awarded a MacArthur Fellowship, often referred to as the Genius Grant, and she has spent the bulk of her career touring the globe to perform her original works. She also credits much of her success to simple daily habits. \"I begin each day of my life with a ritual,\" she writes. \"I wake up at 5.30 a.m., put on my workout clothes, my leg warmers, my sweatshirt, and my hat.\n",
            "{'source': 'txt/Atomic Habits - James Clear.txt'}: You are less likely to procrastinate or give up because there is an immediate cost. If you don't follow through, perhaps they'll see you as untrustworthy or lazy. Suddenly, you are not only failing to uphold your promises to yourself, but also failing to uphold your promises to others. You can even automate this process. Thomas Frank, an entrepreneur in Boulder, Colorado, wakes up at 5.55 each morning. And if he doesn't, he has a tweet automatically scheduled that says \"It's 6/10 and I'm not up because I'm lazy.\" Reply to this for $5 via PayPal, limit 5, assuming my alarm didn't malfunction. We are always trying to present our best selves to the world.\n"
          ]
        }
      ],
      "source": [
        "query_result = db.similarity_search(\"What is procrastination?\", k=4)\n",
        "\n",
        "for chunk in query_result:\n",
        "    print(f\"{chunk.metadata}: {chunk.page_content[:]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CONTINUE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncBRUXkX8xrH"
      },
      "source": [
        "# Chat memory\n",
        "\n",
        "This code imports the [ConversationBufferWindowMemory](https://python.langchain.com/en/latest/modules/memory/types/buffer_window.html) class from the `langchain.memory` module and creates an instance of it called `memory`. This class represents a memory buffer that stores conversations in a windowed fashion, meaning that the buffer only retains a certain number of recent conversations.\n",
        "\n",
        "The constructor of the `ConversationBufferWindowMemory` class takes two arguments: `memory_key` and `return_messages`. The `memory_key` parameter specifies a unique identifier for the memory buffer, and the `return_messages` parameter indicates whether or not to return the stored messages along with their metadata when accessing the memory buffer.\n",
        "\n",
        "In this code, the `memory_key` is set to \"chat_history\", which is being used to store the chat conversations. The return_messages parameter is set to `True`, which indicates that the stored messages will be returned along with their metadata when accessing the memory buffer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "hWbEn5NlBWK7"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "memory = ConversationBufferWindowMemory(memory_key=\"chat_history\", return_messages=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrkpI1EY8t02"
      },
      "source": [
        "# Chain setup\n",
        "\n",
        "This code imports several classes and functions from various modules in the langchain package and creates an instance of the [ConversationalRetrievalChain](https://python.langchain.com/en/latest/modules/chains/index_examples/chat_vector_db.html?highlight=ConversationalRetrievalChain) class called `qa`.\n",
        "\n",
        "The `ConversationalRetrievalChain` class is a high-level class that provides an interface for building a conversational agent that can perform retrieval-based question answering. In this code, the `qa` instance is initialized using the `from_llm()` method, which initializes the agent using an LLM model, a retriever and the memory buffer.\n",
        "\n",
        "### LLM\n",
        "The `OpenAI` class from the `langchain.llms` module represents an instance of the OpenAI language model. In this code, an instance of the OpenAI class is created of the model \"[gpt-3.5-turbo](https://platform.openai.com/docs/models)\".\n",
        "\n",
        "### Vector Store\n",
        "The `faiss_index.as_retriever()` method returns a retriever instance that wraps the FAISS index created earlier. This retriever is used to retrieve candidate answers to questions asked of the conversational agent.\n",
        "\n",
        "### Chat History Memory\n",
        "The `memory` variable is a memory buffer that was created earlier using the `ConversationBufferWindowMemory` class. This memory buffer is used to store and retrieve past conversations for use in future interactions.\n",
        "\n",
        "The `verbose=True` parameter indicates that verbose output should be produced when running the conversational agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "--P_eVczGm1X"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import ConversationalRetrievalChain, LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.chains import RetrievalQA, RetrievalQAWithSourcesChain\n",
        "\n",
        "qa = ConversationalRetrievalChain.from_llm(\n",
        "    OpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7, max_tokens=1000),\n",
        "    db.as_retriever(k=4),\n",
        "     verbose=True) #memory=memory\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZykRyF1TWqsc"
      },
      "source": [
        "# Ask away\n",
        "\n",
        "This code snippet involves a conversational agent that performs question-answering tasks. The user inputs a question, which is passed to the agent as a dictionary with a \"question\" key.\n",
        "\n",
        "The agent then creates an embedding of the question to query the FAISS index and retrieve relevant text chunks based on an internal ranking criterion.\n",
        "\n",
        "Next, the agent makes two calls to the LLM model (\"gpt-3.5-turbo\").\n",
        "\n",
        "- The first call uses the retrieved text chunks, chat history, and the current user question to prompt the LLM to come up with a 'better' question for the entire context.\n",
        "- The second call uses the enhanced question to retrieve the actual answer to the original user question.\n",
        "\n",
        "The resulting answer is stored in the 'chat_result variable', which contains metadata and content related to the answer. The actual answer can be accessed using the \"answer\" key of the 'chat_result' dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "Qb5zcHgA6qDY"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Missing some input keys: {'chat_history'}",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[79], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m query \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mwhat are possible ways to counter procrastination?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[39m#query = \"Wie is Marcus Quintillianus?\"\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m chat_result \u001b[39m=\u001b[39m qa({\u001b[39m\"\u001b[39;49m\u001b[39mquestion\u001b[39;49m\u001b[39m\"\u001b[39;49m: query})\n\u001b[1;32m      5\u001b[0m chat_result\n",
            "File \u001b[0;32m/workspaces/curly-barnacle/.venv/lib/python3.10/site-packages/langchain/chains/base.py:259\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\n\u001b[1;32m    225\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    226\u001b[0m     inputs: Union[Dict[\u001b[39mstr\u001b[39m, Any], Any],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    232\u001b[0m     include_run_info: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    233\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Any]:\n\u001b[1;32m    234\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \n\u001b[1;32m    236\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[39m            `Chain.output_keys`.\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 259\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprep_inputs(inputs)\n\u001b[1;32m    260\u001b[0m     callback_manager \u001b[39m=\u001b[39m CallbackManager\u001b[39m.\u001b[39mconfigure(\n\u001b[1;32m    261\u001b[0m         callbacks,\n\u001b[1;32m    262\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata,\n\u001b[1;32m    268\u001b[0m     )\n\u001b[1;32m    269\u001b[0m     new_arg_supported \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39msignature(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[0;32m/workspaces/curly-barnacle/.venv/lib/python3.10/site-packages/langchain/chains/base.py:413\u001b[0m, in \u001b[0;36mChain.prep_inputs\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    411\u001b[0m     external_context \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory\u001b[39m.\u001b[39mload_memory_variables(inputs)\n\u001b[1;32m    412\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mexternal_context)\n\u001b[0;32m--> 413\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_inputs(inputs)\n\u001b[1;32m    414\u001b[0m \u001b[39mreturn\u001b[39;00m inputs\n",
            "File \u001b[0;32m/workspaces/curly-barnacle/.venv/lib/python3.10/site-packages/langchain/chains/base.py:171\u001b[0m, in \u001b[0;36mChain._validate_inputs\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    169\u001b[0m missing_keys \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_keys)\u001b[39m.\u001b[39mdifference(inputs)\n\u001b[1;32m    170\u001b[0m \u001b[39mif\u001b[39;00m missing_keys:\n\u001b[0;32m--> 171\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing some input keys: \u001b[39m\u001b[39m{\u001b[39;00mmissing_keys\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
            "\u001b[0;31mValueError\u001b[0m: Missing some input keys: {'chat_history'}"
          ]
        }
      ],
      "source": [
        "query = \"what are possible ways to counter procrastination?\"\n",
        "#query = \"Wie is Marcus Quintillianus?\"\n",
        "\n",
        "chat_result = qa({\"question\": query})\n",
        "chat_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
